#This is an image classifier program developed by Andrew J Miller
#This program utilizes convulution Neural Networks
#This program will explain all steps.
#This is primarily for me to teach myself but maybe it will help someone else.
#All credit to CV. Thank you.

"""

-Neural networks are self optimizing programs.
-Their purpose is to run and optimize based on actiavtion
-If you are familiar with biology:
.Think of increasing synaptic cleft surface area
.Maybe you could also think of it as increased axon insulation
-Regardless, the connections optimize by performing an operation
-The operations increase or decrease the opertion when activated
-Sigmoid functions are popular because it fires a yes or no signal
-You can also use other operations but sigmoids are very simple and good.
-Sigmoid: y = 1 / (1 + e^(-z))
-We string operations (neurons) together to get a learner
-The learner then gives us an output
-The connection strength is often referred to as weighted
-All connections are weighed differently
-All layers between the input neurons and the output is hidden
-This is why tensorflow is called tensorflow
-It's because NN's exist in 3 dimensions
-Think of it as a funnel, but not really

Convultional Layer:
-This is basically the math operation layer. 
-We take the input, perform a bunch of calculations
-We then have a reducued layer that is smaller in size than the input (hopefully)
-After convoluting a layer, it gets to the pooled layer
-Pooling is an operation that reduces the size of the matrix
-The goal of pooling is to create less parameters

/break/
I'm sorry, I can only try and make things better. I don't know how to go back yet.
Know that if I knew how I would, know that if I could, I would fix everything.
With the lights on or the lights off, I will do my best to make you proud,
I will do my best to make the world a better place, I will do my best no matter
what it costs. People say it's impossible to make everytone happy, but that
doesn't mean you shouldn't try.
\break\

-The most popular pooling technique is max pooling.

(N - F + 2 * P) / S + 1 = Output size

Stride: S ; input size: w1 * h1 * d1 ; filter size: f * f ; ouput size: w2 * h2 * d2

w2 = (w1 - f) / S + 1
h2 = (h1 - f) / S + 1
d2 = d1

-My guide says that most common pooling is done with a filter of size 2 * 2
-It also says the most common stride is 2

Connected Layers:
-Each neuron in a layer recives information from a neuron in a previous layer
-Outputs are generated by computing matrix multiplication followed by bias offset

Training:
-These are mathematical models of intelligence.
-They mimic human brain computation and connections

Architrcture:
-We have some problems:
.How do we arrage layers?
.How do we decide on the number of neurons that we'll use?
.This area takes a lot of research
.There are many standard archrectures
.But if you want to succeed in this field you're going to need to know more than stadard

Weights/Parameters
-We need to decide how weights and parameters are assigned
-The objective is to get these values right so that we're correct all the time
-Backward propogation is a process that helps optimize parameters and weights
-Your parameters start out random
-Via backprop and gradient descent we can assign the correct weights
-Learning rate defines the magnitude of which you change your weights
-It's very important. It can't be too fast or too slow
-We optimize our learning rate with cost
-We want minimal cost

Cost:
		C = 0.5 * SIG(n, i = 0) (Yactual - Yprediction)^2

-Not all data is fed into the neural network at first the data flows
-Hence the name Tensorflow
-Our data, stored in tensors, flows through the neural net
-A round of training is an epoch

"""

#This isn't the best NN you can use for this, this is simplified
#My laptop unfortunately is not a supercomputer

#Tools:

#Shape function, gives us the shape of the tensor
"""
a = tf.truncated_normal([16,128,128,3])
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(tf.shape(a))
"""

#Reshape function, reshapes our tensors

"""
b = tf.reshape(a, [16, 144])
sess.run(tf.shape(b))
"""

"""Now this is interesting,
We can solve problems in more than 3 dimensions"""

#Softmax function, makes stuff really soft, just kidding,
#It converts a K-dimensional vector to a range 0-->1 the sum of the range is 1
#It's for stats and stuff

#Softmax:

"""
		sm(x)j = (e^xi / SIG( N, n = 1) (e^xn) ) for j = 1 . . . N

Reading inputs:
#We're going to use a images from Kaggle. 
#I'm using 805 images to test this.

#I'm going to train using 66% of my data set and I'll test using 33% of my data set.

"""

#CODE START

classes = ['dogs', 'cats']
num_classes = len(classes)

train_path = 'training_data'
test_path = 'testing_data'

#Validation Split
validation_size = 0.2

#batch size
batch_size = 16

data = datasetImageRead.read_train_sets(train_path, img_size, classes, validation_size = validation_size)
test_images, test_ids = datasetImageRead.read_train_set(test_path,img_size)

#datasetImageRead is a class that reads the input data I hacked it, I did not make it
#It will be uploaded as well.
#You can make your initial weights random
#It's best practice to set them up as a normal distribution

#These funcions will be the new weights/biases

def new_weights(shape):
	return tf.Variable(tf.truncated_normal(shape, stddev = 0.05))

def new_biases(length):
	return tf.Variable(tf.constant(0.05, shape = [length]))

#Creating network layers

#Convolution layer
#tf.nn.conv2d --> creates a 2 NN Layer

#The input should be the output of the prebious layer (also known as the previous layer's activation)
#It should be a 4D tensor
#Filter is defined as the trainable variables, it starts as the random normal distribution
#Filter is a 4D tensor whose shape is defined by network design
#Strides defines how much you move the filter when convoluting 
#In our case the tensor needs to be of size >=4. (batch_stride, x_stride, y_stride, depth_stride)

#Padding = SAME just means that we're going to 0 pad everything
#It has to be 0 padded so our size is the same.

#After convolution the biases are added for that neuron.
#Biases are also learnable these are also a standard normal distribution

#Now we shall apply max pooling 


tf.nn.max_pool(value = layer,
	ksize = [1, 2, 2, 1],
	strides = [1, 2, 2, 1],
	padding = 'SAME')


def new_conv_layer(input, num_input_channels, filter_size, num_filters, use_pooling = True):

	shape = [filter_size, filter_size, num_input_channels, num_filters]

	weights = new_weights(shape = shape)

	biases = new_biases(length = num_filters)

	tf.nn.conv2d(input = input,
		filter = weights,
		strides = [1, 1, 1, 1]
		padding = 'SAME')

	layer += biases

	if use_pooling:
		layer = tf.nn.max_pool(value = layer,
			ksize = [1, 2, 2, 1],
			strides = [1, 2, 2, 1],
			padding = 'SAME')

	layer = tf.nn.relu(layer)
	return layer, weights

#This output is a multi dimensional tensor.

#Now we have a flattening layer. We don't liek multi dim tensors.
#Reshape does this nicely.

def flatten_layer(layer):
	layer_shape = layer.get_shape()
	num_features = layer_shape[1:4].num_elements()

	layer_flat = tf.reshape(layer, [-1, num_features])
	return layer_flat, num_features

#Now a function needs to be made that allows for the layers to be fully connected.
#Weights and bias are normal distributions

#It is at this point that I stopped understanding the guide.
#More on this later.








