{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Andrew J. Miller  - March 28th, 2017 - 22:27\n",
    "\n",
    "#Bowling Green, KY\n",
    "\n",
    "#This is the iPython notebook for the Credit Card Fraud main file. Everything will \n",
    "#be written and explained in this file. I hope that this helps myself better\n",
    "#understand the solution to this problem. Hopefully I will learn something.\n",
    "\n",
    "#Credit: joparga3 for checking my work and providing a framework.\n",
    "\n",
    "#Thanks!\n",
    "#Andy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "5  0.260314 -0.568671  ...   -0.208254 -0.559825 -0.026398 -0.371427   \n",
       "6  0.081213  0.464960  ...   -0.167716 -0.270710 -0.154104 -0.780055   \n",
       "7 -3.807864  0.615375  ...    1.943465 -1.015455  0.057504 -0.649709   \n",
       "8  0.851084 -0.392048  ...   -0.073425 -0.268092 -0.204233  1.011592   \n",
       "9  0.069539 -0.736727  ...   -0.246914 -0.633753 -0.120794 -0.385050   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "5 -0.232794  0.105915  0.253844  0.081080    3.67      0  \n",
       "6  0.750137 -0.257237  0.034507  0.005168    4.99      0  \n",
       "7 -0.415267 -0.051634 -1.206921 -1.085339   40.80      0  \n",
       "8  0.373205 -0.384157  0.011747  0.142404   93.20      0  \n",
       "9 -0.069733  0.094199  0.246219  0.083076    3.68      0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets get the data and take a look.\n",
    "data = pd.read_csv(\"C:/Users/andym/Desktop/AndyMiller/AndyMillerSoftwareDevelopment/DataSets/creditcardfraud.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1903cda4630>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAETCAYAAADge6tNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1903d1c67b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes = pd.value_counts(data['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#So we can't see anything so lets just ask with code:\n",
    "print(ccf['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-So there are 284,315 instances of normal transactions\n",
    "-There are also 492 instances of fraud\n",
    "\n",
    "-Prett imbalanced.\n",
    "-The approach to solve this is called undersampling. Undersampling, in this\n",
    "scenario, just means taking all of the fraud instances and picking an equal\n",
    "number of non-fraud instances. This will make it more obvious to the algorithm\n",
    "what a fraud is vs what a not-fraud is. It also simplifies the data.\n",
    "-So ultimately, we're limited by our smallest classification of data, especially\n",
    "in binary examples. This is may not be obvious at first. But think if you\n",
    "lived in a world with 2,000,000 cats and one dog. The dog would be different,\n",
    "but you probably wouldn't have enough information to classify it as a dog, it'd just be some weird thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest transaction is $ 25691.16 , nice, a car maybe?\n",
      "The smallest transaction is $ 0.0 , \"yes, i'd like to buy one nothing please.\"\n",
      "The largest transaction is $ 88.35 , ok, so like a pair of Nike running shoes.\n"
     ]
    }
   ],
   "source": [
    "#Amount needs to be normalized, lets check out the bigged transaction first.\n",
    "#Let's also find the smallest one and the average.\n",
    "#This is good to know\n",
    "print(\"The largest transaction is $\",ccf['Amount'].max(), \", nice, a car maybe?\")\n",
    "print(\"The smallest transaction is $\",ccf['Amount'].min(), \", \\\"yes, i'd like to buy one nothing please.\\\"\")\n",
    "print(\"The largest transaction is $ %.2f\" % ccf['Amount'].mean(), \", ok, so like a pair of Nike running shoes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest normalized transaction is $ 102.36 , This is the new car\n",
      "The smallest normalized transaction is $ -0.35 , This is the new 0\n",
      "The average normalized transaction is $ 0.00 , This is the new running shoe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andym\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "#So we drop time because it's pretty useless, and we drop the non-normalized amount\n",
    "#I'm going to keep the original data set in memory, but sometimes this isn't a\n",
    "#good idea. change ccfN to ccf if you want to be more efficient.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ccf['nAmount'] = StandardScaler().fit_transform(ccf['Amount'].reshape(-1,1))\n",
    "ccfN=ccf.drop(['Time', 'Amount'],axis=1)\n",
    "ccfN.head(10)\n",
    "\n",
    "print(\"The largest normalized transaction is $ %.2f\" % ccfN['nAmount'].max(), \", This is the new car\")\n",
    "print(\"The smallest normalized transaction is $ %.2f\" %ccfN['nAmount'].min(), \", This is the new 0\")\n",
    "print(\"The average normalized transaction is $ %.2f\" % ccfN['nAmount'].mean(), \", This is the new running shoe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to notice the change in the average. The 32.36 is just for fun, but we can figure that the new max is around the 7500 dollar, the minimum is obviously probably a couple of cents, and the mean is around the 95 dollar range. So a bit skewed but not too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Non-Fraud in Undersampled Data:  0.5\n",
      "Percent Fraud in Undersampled Data:  0.5\n",
      "Total Cases:  984\n",
      "The largest normalized transaction is $ 11.91 , This is the new car\n",
      "The smallest normalized transaction is $ -0.35 , This is the new 0\n",
      "The average normalized transaction is $ 0.08 , This is the new running shoe\n"
     ]
    }
   ],
   "source": [
    "#Samples in the tiny class\n",
    "fraudCases = len(ccfN[ccfN.Class==1]) #fraud = 492 pieces\n",
    "fraudIndices = np.array(ccfN[ccfN.Class==1].index) #tells us the row in array format. For Fraud\n",
    "\n",
    "nonFraud = len(ccfN[ccfN.Class==0]) #non-fraud = 284,315 pieces\n",
    "nonFraudIndices = np.array(ccfN[ccfN.Class==0].index) #tells us the row in array format. For non-fraud\n",
    "\n",
    "#Great but we need an undersampled version of the non-Fraud indices\n",
    "randNonFraudIndices = np.random.choice(nonFraudIndices, fraudCases, replace=False) #Generates 492 random non-fraud case indices for us.\n",
    "\n",
    "#Combining and pulling data\n",
    "UnderSampledIndices = np.concatenate([randNonFraudIndices, fraudIndices]) #Combines the indices.\n",
    "underSampled_ccfN = ccfN.iloc[UnderSampledIndices,:] #Grabs us the data that our undersampled indices points to\n",
    "\n",
    "#For training and testing large data:\n",
    "x=ccfN.ix[:,ccfN.columns != 'Class']\n",
    "y=ccfN.ix[:,ccfN.columns == 'Class']\n",
    "#Creates 2 data sets, one with class and indices, (y), and one with attributes and no class, (x)\n",
    "\n",
    "#Testing new data:\n",
    "xU=underSampled_ccfN.ix[:,underSampled_ccfN.columns != 'Class'] #x from above just undersampled\n",
    "yU=underSampled_ccfN.ix[:,underSampled_ccfN.columns == 'Class'] #y from above just undersampled\n",
    "\n",
    "#Checking new dataset, compare this with the old data.\n",
    "print(\"Percent Non-Fraud in Undersampled Data: \", len(randNonFraudIndices)/len(underSampled_ccfN))\n",
    "print(\"Percent Fraud in Undersampled Data: \", len(fraudIndices)/len(underSampled_ccfN))\n",
    "print(\"Total Cases: \", len(underSampled_ccfN))\n",
    "print(\"The largest normalized transaction is $ %.2f\" % underSampled_ccfN['nAmount'].max(), \", This is the new car\")\n",
    "print(\"The smallest normalized transaction is $ %.2f\" % underSampled_ccfN['nAmount'].min(), \", This is the new 0\")\n",
    "print(\"The average normalized transaction is $ %.2f\" % underSampled_ccfN['nAmount'].mean(), \", This is the new running shoe\")\n",
    "#It's important to check out the new mean. That is our change in the data due to undersampling\n",
    "#That is going to cause some form of innacuracy to develop.\n",
    "#Just a good note, I'm not sure how to deal with it yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Debugging Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big data set:\n",
      "number of x training:  189966\n",
      "number of y training:  189966\n",
      "number of x testing:  94841\n",
      "number of y testing:  94841\n",
      "Total x:  284807\n",
      "Total y:  284807\n",
      " \n",
      "Baby data set:\n",
      "undersampled number of x training:  656\n",
      "undersampled number of y training:  656\n",
      "undersampled number of x testing:  328\n",
      "undersampled number of y testing:  328\n",
      "undersampled Total x:  984\n",
      "undersampled Total y:  984\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Let us train, test, split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Big data set:\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y,test_size=0.333)\n",
    "\n",
    "#checking lengths to make sure they're right:\n",
    "print(\"Big data set:\")\n",
    "print(\"number of x training: \", len(xTrain))\n",
    "print(\"number of y training: \",len(yTrain))\n",
    "\n",
    "print(\"number of x testing: \", len(xTest))\n",
    "print(\"number of y testing: \",len(yTest))\n",
    "\n",
    "print(\"Total x: \", len(xTrain)+len(xTest))\n",
    "print(\"Total y: \",len(yTrain)+len(yTest))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "#Undersampled data set:\n",
    "xUTrain, xUTest, yUTrain, yUTest = train_test_split(xU,yU,test_size=0.333)\n",
    "\n",
    "#checking lengths to make sure they're right:\n",
    "print(\"Baby data set:\")\n",
    "print(\"undersampled number of x training: \", len(xUTrain))\n",
    "print(\"undersampled number of y training: \",len(yUTrain))\n",
    "\n",
    "print(\"undersampled number of x testing: \", len(xUTest))\n",
    "print(\"undersampled number of y testing: \",len(yUTest))\n",
    "\n",
    "print(\"undersampled Total x: \", len(xUTrain)+len(xUTest))\n",
    "print(\"undersampled Total y: \",len(yUTrain)+len(yUTest))\n",
    "print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression and Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printing_Kfold_scores(x_train_data,y_train_data):\n",
    "    fold = KFold(len(y_train_data),5,shuffle=False) \n",
    "\n",
    "    # Different C parameters\n",
    "    c_param_range = [0.01,0.1,1,10,100]\n",
    "\n",
    "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "\n",
    "    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n",
    "    j = 0\n",
    "    for c_param in c_param_range:\n",
    "        print('-------------------------------------------')\n",
    "        print('C parameter: ', c_param)\n",
    "        print('-------------------------------------------')\n",
    "        print('')\n",
    "\n",
    "        recall_accs = []\n",
    "        for iteration, indices in enumerate(fold,start=1):\n",
    "\n",
    "            # Call the logistic regression model with a certain C parameter\n",
    "            lr = LogisticRegression(C = c_param, penalty = 'l1')\n",
    "\n",
    "            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model\n",
    "            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]\n",
    "            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n",
    "\n",
    "            # Predict values using the test indices in the training data\n",
    "            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n",
    "\n",
    "            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter\n",
    "            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
    "            recall_accs.append(recall_acc)\n",
    "            print('Iteration ', iteration,': recall score = ', recall_acc)\n",
    "\n",
    "        # The mean value of those recall scores is the metric we want to save and get hold of.\n",
    "        results_table.ix[j,'Mean recall score'] = np.mean(recall_accs)\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('Mean recall score ', np.mean(recall_accs))\n",
    "        print('')\n",
    "\n",
    "    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print('*********************************************************************************')\n",
    "    print('Best model to choose from cross validation is with C parameter = ', best_c)\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    return best_c\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-eeaa099f33a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbestC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprinting_Kfold_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxUTrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myUTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-121a56fabf2f>\u001b[0m in \u001b[0;36mprinting_Kfold_scores\u001b[0;34m(x_train_data, y_train_data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprinting_Kfold_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;31m# Different C parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mc_param_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'shuffle'"
     ]
    }
   ],
   "source": [
    "bestC = printing_Kfold_scores(xUTrain,yUTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
