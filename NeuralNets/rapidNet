#This is a rapid set-up for a neural network
#It has 3 layers
#This is the skeleton base for all of my neural nets

import numpy as np


#neuron set-up (sigmoid)
#neurons can have different non-linear types
def nonlin(x,deriv=False):
	if (deriv==True):
		return x*(1-x)

	return 1/(1+np.exp(-x))

#input, column 3 doesn't exist and is there because it is necessary for the bias term.
X = np.array([[0,0,1],
            [0,1,1],
            [1,0,1],
            [1,1,1]])

#output
y = np.array([[0],
             [1],
             [1],
             [0]])

#np.random.seed(1)


#synapses
syn0 = 2*np.random.random((3,4)) - 1 #This is a 3x4 matrix of weights (2in & 1 bias) 4 nodes in hidden layer
syn1 = 2*np.random.random((4,1)) - 1 #This is a 4x1 matrix of weights (4 nodes % 1 output) - no bias

#training
for j in xrange(60000):
	l0=X
	l1=nonlin(np.dot(l0,syn0))
	l2=nonlin(np.dot(l1,syn1))

	l2_error = y - l2

	if (j % 60000) == 0:
		print ("error:" + str(np.mean(np.abs(l2_error)))) #This should be decreasing as the net learns.

	l2_delta = l2_error*nonlin(l2,deriv=True)

	l1_error = l2_delta.dot(syn1.T)

	l1_delta = l1_error*nonlin(l1,deriv=True)

	#weight updating
	syn1 += l1.T.dot(l2_delta)
	syn0 += l0.T.dot(l1_delta)

print ("output after training:")
print (l2) 